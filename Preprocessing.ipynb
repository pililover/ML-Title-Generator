{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "## G·ªôp c√°c d·ªØ li·ªáu thu th·∫≠p ƒë∆∞·ª£c v√†o m·ªôt file duy nh·∫•t, lo·∫°i b·ªè c√°c d√≤ng ch·ª©a gi√° tr·ªã NULL v√† x√≥a c√°c d·ªØ li·ªáu b·ªã l·∫∑p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang x·ª≠ l√Ω file: dataraw\\data_thanhnien.csv\n",
      "üìÇ ƒêang x·ª≠ l√Ω file: dataraw\\tuoitretotal.csv\n",
      "üìÇ ƒêang x·ª≠ l√Ω file: dataraw\\vnexpress_articles.csv\n",
      "\n",
      "‚úÖ T·ªïng s·ªë d√≤ng sau khi l√†m s·∫°ch v√† g·ªôp: 1339798\n",
      "üìÇ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: data_process_1\\data_no_null.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_merge_csv_files(input_folder, file_names, output_folder, output_file, chunk_size=50000):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    merged_df = pd.DataFrame() \n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"üìÇ ƒêang x·ª≠ l√Ω file: {file_path}\")\n",
    "\n",
    "        # ƒê·ªçc file theo chunksize\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size, dtype=str):\n",
    "            # Chu·∫©n h√≥a t√™n c·ªôt (chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè kho·∫£ng tr·∫Øng)\n",
    "            chunk.columns = [col.lower().strip() for col in chunk.columns]\n",
    "\n",
    "            # üõë X√≥a to√†n b·ªô d√≤ng c√≥ gi√° tr·ªã NULL ·ªü b·∫•t k·ª≥ c·ªôt n√†o\n",
    "            chunk_clean = chunk.dropna()\n",
    "\n",
    "            # G·ªôp d·ªØ li·ªáu v√†o DataFrame ch√≠nh\n",
    "            merged_df = pd.concat([merged_df, chunk_clean], ignore_index=True)\n",
    "\n",
    "    # X√≥a tr√πng l·∫∑p d·ª±a tr√™n c·ªôt 'title'\n",
    "    merged_df.drop_duplicates(subset=['title'], inplace=True)\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    output_path = os.path.join(output_folder, output_file)\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ T·ªïng s·ªë d√≤ng sau khi l√†m s·∫°ch v√† g·ªôp: {merged_df.shape[0]}\")\n",
    "    print(f\"üìÇ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}\")\n",
    "\n",
    "# Th∆∞ m·ª•c ch·ª©a t·ªáp ƒë·∫ßu v√†o v√† t√™n c√°c t·ªáp c·∫ßn x·ª≠ l√Ω\n",
    "input_folder = \"dataraw\"\n",
    "files = [\"data_thanhnien.csv\", \"tuoitretotal.csv\", \"vnexpress_articles.csv\"]\n",
    "\n",
    "# Th∆∞ m·ª•c v√† t·ªáp ƒë·∫ßu ra\n",
    "output_folder = \"data_process_1\"\n",
    "output_file = \"data_no_null.csv\"\n",
    "\n",
    "# G·ªçi h√†m\n",
    "clean_and_merge_csv_files(input_folder, files, output_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ki·ªÉm tra l·∫°i data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõë S·ªë gi√° tr·ªã NULL tr√™n t·ª´ng c·ªôt trong to√†n b·ªô d·ªØ li·ªáu:\n",
      "url               0\n",
      "title             0\n",
      "abstract          0\n",
      "content           0\n",
      "category      38688\n",
      "genre       1301110\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"data_process_1/data_no_null.csv\"\n",
    "chunk_size = 50000  \n",
    "\n",
    "null_counts = None\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    if null_counts is None:\n",
    "        null_counts = chunk.isnull().sum()  \n",
    "    else:\n",
    "        null_counts += chunk.isnull().sum()  \n",
    "\n",
    "print(\"\\nüõë S·ªë gi√° tr·ªã NULL tr√™n t·ª´ng c·ªôt trong to√†n b·ªô d·ªØ li·ªáu:\")\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lo·∫°i b·ªè c·ªôt genre, x√≥a c√°c d√≤ng c√≥ ch·ª©a gi√° tr·ªã c·ªôt category tr·ªëng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† l∆∞u l·∫°i t·∫°i: data_process_2\\data_no_genre_and_category_null.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n file g·ªëc\n",
    "file_path = \"data_process_1/data_no_null.csv\"\n",
    "output_folder = \"data_process_2\"\n",
    "output_file = \"data_no_genre_and_category_null.csv\"\n",
    "chunk_size = 50000  # X·ª≠ l√Ω t·ª´ng ph·∫ßn nh·ªè\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, output_file)\n",
    "\n",
    "# X·ª≠ l√Ω d·ªØ li·ªáu theo chunks\n",
    "df_clean = pd.DataFrame() \n",
    "\n",
    "with pd.read_csv(file_path, chunksize=chunk_size) as reader:\n",
    "    for chunk in reader:\n",
    "        chunk_clean = chunk.dropna(subset=['category'])\n",
    "        chunk_clean = chunk_clean.drop(columns=['genre'])\n",
    "        # G·ªôp d·ªØ li·ªáu \n",
    "        df_clean = pd.concat([df_clean, chunk_clean], ignore_index=True)\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£ v√†o file m·ªõi trong th∆∞ m·ª•c data_process_2\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† l∆∞u l·∫°i t·∫°i: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ki·ªÉm tra l·∫°i d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõë S·ªë gi√° tr·ªã NULL tr√™n t·ª´ng c·ªôt trong to√†n b·ªô d·ªØ li·ªáu:\n",
      "url         0\n",
      "title       0\n",
      "abstract    0\n",
      "content     0\n",
      "category    0\n",
      "dtype: int64\n",
      "\n",
      "üìù S·ªë d√≤ng tr√πng l·∫∑p theo c·ªôt 'title': 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"data_process_2/data_no_genre_and_category_null.csv\"\n",
    "chunk_size = 50000  \n",
    "\n",
    "null_counts = None\n",
    "title_duplicates = 0  # Bi·∫øn ƒë·∫øm s·ªë d√≤ng tr√πng l·∫∑p theo c·ªôt 'title'\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Ki·ªÉm tra gi√° tr·ªã NULL\n",
    "    if null_counts is None:\n",
    "        null_counts = chunk.isnull().sum()\n",
    "    else:\n",
    "        null_counts += chunk.isnull().sum()\n",
    "\n",
    "    # Ki·ªÉm tra c√°c d√≤ng tr√πng l·∫∑p theo c·ªôt 'title'\n",
    "    title_duplicates += chunk.duplicated(subset=['title']).sum()\n",
    "\n",
    "print(\"\\nüõë S·ªë gi√° tr·ªã NULL tr√™n t·ª´ng c·ªôt trong to√†n b·ªô d·ªØ li·ªáu:\")\n",
    "print(null_counts)\n",
    "\n",
    "print(f\"\\nüìù S·ªë d√≤ng tr√πng l·∫∑p theo c·ªôt 'title': {title_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuy·ªÉn c·ªôt category th√†nh ch·ªØ th∆∞·ªùng v√† ki·ªÉm tra s·ªë th·ªÉ lo·∫°i ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä S·ªë l∆∞·ª£ng d√≤ng c·ªßa t·ª´ng lo·∫°i 'category':\n",
      "'th·ªùi s·ª±': 57\n",
      "'c√¥ng ngh·ªá': 966\n",
      "'th·ªÉ thao': 142\n",
      "'th·∫ø gi·ªõi': 90\n",
      "'vƒÉn h√≥a': 68\n",
      "'kinh t·∫ø': 2228\n",
      "'video': 2452\n",
      "'gi·∫£i tr√≠': 1\n",
      "'s·ª©c kh·ªèe': 46\n",
      "'gi√°o d·ª•c': 22\n",
      "'ƒë·ªùi s·ªëng': 1222\n",
      "'gi·ªõi tr·∫ª': 822\n",
      "'tno': 4\n",
      "'xe': 12\n",
      "'du l·ªãch': 12\n",
      "'ti√™u d√πng th√¥ng minh': 365\n",
      "'ch√≠nh tr·ªã': 708\n",
      "'b·∫°n c·∫ßn bi·∫øt': 302\n",
      "'b·∫°n ƒë·ªçc': 198\n",
      "'ch√†o ng√†y m·ªõi': 81\n",
      "'blog ph√≥ng vi√™n': 41\n",
      "'podcast': 2\n",
      "'t√¥i vi·∫øt': 11\n",
      "'di·ªÖn ƒë√†n': 43\n",
      "'nh·∫≠t k√Ω t·∫øt vi·ªát': 27\n",
      "'kinh doanh': 84\n",
      "'ph√°p lu·∫≠t': 44\n",
      "'b·∫°n ƒë·ªçc l√†m b√°o': 44\n",
      "'c·∫ßn bi·∫øt': 12\n",
      "'nh·ªãp s·ªëng tr·∫ª': 67\n",
      "'kinh doanh, doanh nghi·ªáp': 148\n",
      "'nh√† ƒë·∫•t': 19\n",
      "'th·ªùi s·ª±, th·ªùi ti·∫øt': 1\n",
      "'khoa h·ªçc': 11\n",
      "'th·ªùi s·ª±, ph√≥ng s·ª±': 21\n",
      "'gi·∫£ - th·∫≠t': 222\n",
      "'th·ªùi s·ª±, b√¨nh lu·∫≠n': 10\n",
      "'kinh doanh, t√†i ch√≠nh': 13\n",
      "'s·ª©c kh·ªèe, bi·∫øt ƒë·ªÉ kh·ªèe': 1000\n",
      "'kinh doanh, mua s·∫Øm': 26\n",
      "'th·ªùi s·ª±, x√£ h·ªôi': 201\n",
      "'th·ªùi s·ª±, b√∫t bi': 144\n",
      "'kinh doanh, ƒë·∫ßu t∆∞': 42\n",
      "'nh·ªãp s·ªëng tr·∫ª, tu·ªïi tr·∫ª start-up award': 22\n",
      "'nh√† ƒë·∫•t, d·ª± √°n': 3\n",
      "'gi√°o d·ª•c, nh·ªãp s·ªëng h·ªçc ƒë∆∞·ªùng': 200\n",
      "'c√¥ng ngh·ªá, nh·ªãp s·ªëng s·ªë': 25\n",
      "'gi√°o d·ª•c, tuy·ªÉn sinh': 14\n",
      "'du l·ªãch, c∆° h·ªôi du l·ªãch': 64\n",
      "'th·ªÉ thao, b√≥ng ƒë√°': 2\n",
      "'du l·ªãch, ƒëi ch∆°i': 4\n",
      "'media, c√¥ng ngh·ªá': 3\n",
      "'vi·ªác l√†m': 75\n",
      "'xe, th·ªã tr∆∞·ªùng xe': 1\n",
      "'s·ª©c kh·ªèe, dinh d∆∞·ª°ng': 35\n",
      "'nh·ªãp s·ªëng tr·∫ª, xu h∆∞·ªõng': 47\n",
      "'th·∫ø gi·ªõi, mu√¥n m√†u': 24\n",
      "'gi√°o d·ª•c, c√¢u chuy·ªán gi√°o d·ª•c': 15\n",
      "'gi·∫£i tr√≠, √¢m nh·∫°c': 8\n",
      "'ph√°p lu·∫≠t, t∆∞ v·∫•n': 2\n",
      "'nh·ªãp s·ªëng tr·∫ª, theo g∆∞∆°ng b√°c': 8\n",
      "'c√¥ng ngh·ªá, thi·∫øt b·ªã': 2\n",
      "'gi·∫£i tr√≠, ƒëi·ªán ·∫£nh': 12\n",
      "'c√¥ng ngh·ªá, th·ªã tr∆∞·ªùng': 9\n",
      "'nh√† ƒë·∫•t, ch√≠nh s√°ch': 1\n",
      "'ph√°p lu·∫≠t, chuy·ªán ph√°p ƒë√¨nh': 1\n",
      "'b·∫°n ƒë·ªçc l√†m b√°o, ph·∫£n h·ªìi': 39\n",
      "'c√¥ng ngh·ªá, c·∫ßu n·ªëi': 1\n",
      "'xe, tin t·ª©c': 1\n",
      "'c√¥ng ngh·ªá, chuy·ªÉn ƒë·ªïi s·ªë': 11\n",
      "'gi√°o d·ª•c, du h·ªçc': 12\n",
      "'gi√°o d·ª•c, ch√¢n dung nh√† gi√°o': 8\n",
      "'vƒÉn h√≥a, ƒë·ªùi s·ªëng': 5\n",
      "'vƒÉn h√≥a, ·∫©m th·ª±c': 5\n",
      "'c√¥ng ngh·ªá, nh·ªãp c·∫ßu': 1\n",
      "'nh√† ƒë·∫•t, th·ªã tr∆∞·ªùng': 1\n",
      "'s·ª©c kh·ªèe, m·∫π & b√©': 36\n",
      "'th·ªÉ thao, c√°c m√¥n kh√°c': 17\n",
      "'ph√°p lu·∫≠t, ph√°p l√Ω': 9\n",
      "'c√¥ng ngh·ªá, tr·∫£i nghi·ªám': 1\n",
      "'s·ª©c kh·ªèe, ph√≤ng m·∫°ch': 4\n",
      "'vi·ªác l√†m, ƒë·ªãnh h∆∞·ªõng v√† t∆∞ v·∫•n ngh·ªÅ nghi·ªáp': 14\n",
      "'vi·ªác l√†m, c·∫©m nang vi·ªác l√†m': 7\n",
      "'b·∫°n ƒë·ªçc l√†m b√°o, ƒë∆∞·ªùng d√¢y n√≥ng': 59\n",
      "'nh·ªãp s·ªëng tr·∫ª, nh√¢n v·∫≠t': 1\n",
      "'b·∫°n ƒë·ªçc l√†m b√°o, chia s·∫ª': 4\n",
      "'th·ªÉ thao, b√≥ng r·ªï': 1\n",
      "'vi·ªác l√†m, vi·∫øt cv & ph·ªèng v·∫•n': 1\n",
      "'vi·ªác l√†m, g√≥c k·ªπ nƒÉng': 1\n",
      "'bi·∫øt ƒë·ªÉ kh·ªèe, l√†m ƒë·∫πp': 2\n",
      "'b·∫°n ƒë·ªçc l√†m b√°o, ti√™u ƒëi·ªÉm': 1\n",
      "'du l·ªãch, m√°ch b·∫°n': 1\n",
      "'c·∫ßn bi·∫øt, h·ªçc h√†nh': 1\n",
      "'th·∫ø gi·ªõi, b√¨nh lu·∫≠n': 31\n",
      "'gi·∫£i tr√≠, th·ªùi trang': 3\n",
      "'nh·ªãp s·ªëng tr·∫ª, kh√°m ph√°': 1\n",
      "'du l·ªãch, m√≥n ngon': 3\n",
      "'c·∫ßn bi·∫øt, ƒëi·ªÉm tin c√πng b·∫°n': 1\n",
      "'nh·ªãp s·ªëng tr·∫ª, y√™u': 10\n",
      "'photo media': 1\n",
      "'du l·ªãch, qu√™ h∆∞∆°ng': 1\n",
      "'th·ªÉ thao, v√µ thu·∫≠t': 1\n",
      "'vƒÉn h√≥a, s√†n di·ªÖn': 4\n",
      "'vƒÉn h√≥a, nh√¢n v·∫≠t': 1\n",
      "'nh·ªãp s·ªëng tr·∫ª, vi·ªác l√†m': 5\n",
      "'nh√† ƒë·∫•t, th·∫ø gi·ªõi': 2\n",
      "'mekong xanh, m√¥i tr∆∞·ªùng': 1\n",
      "'th·ªÉ thao, l·ªãch thi ƒë·∫•u': 1\n",
      "'du l·ªãch, kh√°ch s·∫°n': 1\n",
      "'th·ªÉ thao, kh·ªèe 360': 2\n",
      "'xe, ƒë√°nh gi√° xe': 1\n",
      "'world cup 2022': 480\n",
      "'media': 2\n",
      "'th·ªÉ thao, sea games 31': 51\n",
      "'sea games 31, b√≥ng ƒë√°': 142\n",
      "'c·∫ßn bi·∫øt, ƒë·ªùi s·ªëng': 6\n",
      "'vƒÉn h√≥a, s√°ch': 42\n",
      "'gi·∫£i tr√≠, h·∫≠u tr∆∞·ªùng': 6\n",
      "'bi·∫øt ƒë·ªÉ kh·ªèe, ph√≤ng b·ªánh': 7\n",
      "'th·ªÉ thao, ng∆∞·ªùi h√¢m m·ªô': 29\n",
      "'world cup 2022, th∆∞ qatar': 2\n",
      "'xe, k·ªπ thu·∫≠t': 2\n",
      "'th·∫ø gi·ªõi, ki·ªÅu b√†o': 6\n",
      "'gi·∫£i tr√≠, tv show': 11\n",
      "'th·ªÉ thao, b√¨nh lu·∫≠n': 2\n",
      "'xe, an to√†n giao th√¥ng': 1\n",
      "'media, th·ªùi s·ª±': 28\n",
      "'sea games 31, ng∆∞·ªùi h√¢m m·ªô': 73\n",
      "'c·∫ßn bi·∫øt, th·ªã tr∆∞·ªùng 247': 4\n",
      "'th·ªÉ thao, video': 4\n",
      "'khoa h·ªçc, ph√°t minh': 15\n",
      "'bi·∫øt ƒë·ªÉ kh·ªèe, dinh d∆∞·ª°ng': 2\n",
      "'tu·ªïi tr·∫ª cu·ªëi tu·∫ßn, vƒÉn h√≥a - gi·∫£i tr√≠': 1\n",
      "'c√¥ng ngh·ªá, gia ƒë√¨nh s·ªë': 10\n",
      "'video media': 1\n",
      "'c·∫ßn bi·∫øt, ƒë·ªãa ·ªëc': 3\n",
      "'gi·∫£i tr√≠, nghe g√¨ h√¥m nay': 34\n",
      "'c·∫ßn bi·∫øt, gi·∫£i tr√≠': 4\n",
      "'th·∫ø gi·ªõi, h·ªì s∆°': 45\n",
      "'khoa h·ªçc, th∆∞·ªùng th·ª©c': 7\n",
      "'th∆∞ gi√£n': 33\n",
      "'th·ªÉ thao, world cup 2018': 254\n",
      "'world cup 2018, world cup trong m·∫Øt t√¥i': 12\n",
      "'sea games 31, c√°c m√¥n kh√°c': 34\n",
      "'ƒë·∫°i h·ªçc, c√πng b·∫°n ch·ªçn tr∆∞·ªùng': 10\n",
      "'c·∫ßn bi·∫øt, s·∫£n ph·∫©m': 2\n",
      "'sea games 31, ng√¥i sao sea games': 24\n",
      "'sea games 31, theo ch√¢n ph√≥ng vi√™n': 24\n",
      "'sea games 31, d·ª± ƒëo√°n c·∫ßu th·ªß xu·∫•t s·∫Øc nh·∫•t': 23\n",
      "'ƒë·∫°i h·ªçc, ngh·ªÅ c·∫ßn ng∆∞·ªùi': 1\n",
      "'b√¨nh d∆∞∆°ng': 9\n",
      "'ƒë·∫°i h·ªçc, x√©t tuy·ªÉn ch·ªçn ng√†nh': 5\n",
      "'bi·∫øt ƒë·ªÉ kh·ªèe, kh√°m ƒëi·ªÅu tr·ªã': 4\n",
      "'c√¥ng ngh·ªá, th·ªß thu·∫≠t': 2\n",
      "'world cup 2018, b√¨nh lu·∫≠n': 10\n",
      "'media, th·ªÉ thao': 2\n",
      "'mekong xanh, chuy·ªán mekong': 1\n",
      "'nh√† ƒë·∫•t, smarthome': 12\n",
      "'world cup 2018, tin t·ª©c': 52\n",
      "'du l·ªãch, c√¥ng ty du l·ªãch': 7\n",
      "'world cup 2018, b√™n l·ªÅ': 12\n",
      "'world cup 2018, ch√¢n dung 32 ƒë·ªôi': 7\n",
      "'du l·ªãch, nh√† h√†ng': 3\n",
      "'media, gi·ªõi tr·∫ª': 3\n",
      "'media, gi·∫£i tr√≠': 2\n",
      "'b√¨nh d∆∞∆°ng, t·∫ßm nh√¨n': 2\n",
      "'b√¨nh d∆∞∆°ng, smart city': 2\n",
      "'du l·ªãch, box ·∫£nh': 2\n",
      "'b√¨nh d∆∞∆°ng, k·∫øt n·ªëi': 2\n",
      "'world cup 2018, cu·ªôc thi vi·∫øt': 1\n",
      "'vƒÉn h√≥a - gi·∫£i tr√≠, s√°ng t√°c - truy·ªán ng·∫Øn': 2\n",
      "'tu·ªïi tr·∫ª cu·ªëi tu·∫ßn, ƒëi·ªÉm n√≥ng': 8\n",
      "'cu·ªôc s·ªëng mu√¥n m√†u, nh·∫≠t k√Ω th√†nh ph·ªë': 1\n",
      "'cu·ªôc s·ªëng mu√¥n m√†u, c√¢u chuy·ªán cu·ªôc s·ªëng': 3\n",
      "'tu·ªïi tr·∫ª cu·ªëi tu·∫ßn, v·∫•n ƒë·ªÅ - s·ª± ki·ªán': 3\n",
      "'s·ª©c kh·ªèe, gi·ªõi t√≠nh': 17\n",
      "'v·∫•n ƒë·ªÅ - s·ª± ki·ªán, ti√™u ƒëi·ªÉm': 1\n",
      "'cu·ªôc s·ªëng mu√¥n m√†u, l√° th∆∞ b√°c sƒ© - s·ª©c kh·ªèe': 1\n",
      "'cu·ªôc s·ªëng mu√¥n m√†u, phoÃÅng s∆∞Ã£ - phoÃÅng s∆∞Ã£ aÃânh': 1\n",
      "'v·∫•n ƒë·ªÅ - s·ª± ki·ªán, th·ªùi s·ª± qu·ªëc t·∫ø': 1\n",
      "'cu·ªôc s·ªëng mu√¥n m√†u, ph√≥ng s·ª± - h·ªì s∆°': 1\n",
      "'vƒÉn h√≥a - gi·∫£i tr√≠, th·ªÉ thao': 1\n",
      "'du l·ªãch, ph√≤ng v√©': 3\n",
      "'v·∫•n ƒë·ªÅ - s·ª± ki·ªán, b·∫°n ƒë·ªçc': 2\n",
      "'vƒÉn h√≥a - gi·∫£i tr√≠, th∆∞ gi√£n': 1\n",
      "'vƒÉn h√≥a - gi·∫£i tr√≠, t·∫°p b√∫t': 1\n",
      "\n",
      "‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l∆∞u t·∫°i: data_process_3\\data_no_genre_and_category_lower.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu\n",
    "file_path = \"data_process_2/data_no_genre_and_category_null.csv\"\n",
    "output_folder = \"data_process_3\"\n",
    "output_file = \"data_no_genre_and_category_lower.csv\"\n",
    "\n",
    "chunk_size = 50000\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, output_file)\n",
    "\n",
    "# L∆∞u s·ªë l∆∞·ª£ng d√≤ng theo t·ª´ng lo·∫°i category\n",
    "category_counts = {}\n",
    "\n",
    "# ƒê·ªçc t·ªáp d·ªØ li·ªáu theo t·ª´ng chunk\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Chuy·ªÉn c·ªôt 'category' v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "    chunk['category'] = chunk['category'].str.lower()\n",
    "    \n",
    "    # Ki·ªÉm tra s·ªë d√≤ng c·ªßa m·ªói lo·∫°i category\n",
    "    category_counts_chunk = chunk['category'].value_counts()\n",
    "    \n",
    "    # C·ªông d·ªìn v√†o s·ªë l∆∞·ª£ng category ƒë√£ c√≥\n",
    "    category_counts.update(category_counts_chunk.to_dict())\n",
    "\n",
    "    # Ghi d·ªØ li·ªáu sau khi ƒë√£ chuy·ªÉn c·ªôt category v·ªÅ ch·ªØ th∆∞·ªùng v√†o file m·ªõi\n",
    "    chunk.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)\n",
    "\n",
    "# In s·ªë l∆∞·ª£ng d√≤ng c·ªßa t·ª´ng lo·∫°i category\n",
    "print(\"\\nüìä S·ªë l∆∞·ª£ng d√≤ng c·ªßa t·ª´ng lo·∫°i 'category':\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"'{category}': {count}\")\n",
    "\n",
    "print(f\"\\n‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l∆∞u t·∫°i: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PhatK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# T·∫£i l·∫°i t√†i nguy√™n 'punkt'\n",
    "nltk.download('punkt', force=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
