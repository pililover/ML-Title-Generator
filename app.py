import streamlit as st
import os
import time
from datetime import datetime
from underthesea import word_tokenize
from transformers import EncoderDecoderModel, AutoModelForSeq2SeqLM, AutoTokenizer
import torch
import logging
import transformers
import google.generativeai as genai
from utils.preprocessing import clean_text, segment_text
import asyncio

try:
    asyncio.get_running_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())
    
# Gi·∫£m b·ªõt c·∫£nh b√°o
logging.getLogger('streamlit.runtime.scriptrunner.script_run_context').setLevel(logging.ERROR)
transformers.logging.set_verbosity_error()

# C·∫•u h√¨nh Streamlit
st.set_page_config(page_title="Tr√¨nh sinh ti√™u ƒë·ªÅ", layout="centered")

# C·∫•u h√¨nh Gemini API (thay YOUR_GEMINI_API_KEY b·∫±ng API key th·ª±c t·∫ø)
GEMINI_API_KEY = "AIzaSyCEDRquPDC9N09hTHGD9FfvsPP83AZT78Q"  # Thay b·∫±ng API key th·ª±c t·∫ø c·ªßa b·∫°n
genai.configure(api_key=GEMINI_API_KEY)

# C√°c m√¥ h√¨nh
TITLE_MODELS = {
    "PhoBERT Encoder-Decoder": {
        "model_path": "PuppetLover/Title_generator",
        "tokenizer_path": "vinai/phobert-base-v2",
        "token": True,
        "model_type": "encoder-decoder"
    },
    "ViT5 Title Generator": {
        "model_path": "HTThuanHcmus/vit5-base-vietnews-summarization-finetune",
        "tokenizer_path": "HTThuanHcmus/vit5-base-vietnews-summarization-finetune",
        "token": False,
        "model_type": "seq2seq"
    },
    "BARTpho Title Generator": {
        "model_path": "HTThuanHcmus/bartpho-finetune",
        "tokenizer_path": "HTThuanHcmus/bartpho-finetune",
        "token": False,
        "model_type": "seq2seq"
    },
    "Gemini Title Generator": {
        # "model_path": "gemini-1.5-pro",
        "model_path" : "gemini-1.5-flash",
        "tokenizer_path": None,
        "token": False,
        "model_type": "gemini"
    }
}

SUMMARIZATION_MODELS = {
    "ViT5 Summarization": {
        "model_path": "HTThuanHcmus/vit5-summarization-news-finetune",
        "tokenizer_path": "HTThuanHcmus/vit5-summarization-news-finetune",
        "token": False,
        "model_type": "seq2seq"
    },
    "BARTpho Summarization": {
        "model_path": "HTThuanHcmus/bartpho-summarization-news-finetune",
        "tokenizer_path": "HTThuanHcmus/bartpho-summarization-news-finetune",
        "token": False,
        "model_type": "seq2seq"
    },
    "Gemini Summarization": {
        "model_path": "gemini-1.5-pro",
        "tokenizer_path": None,
        "token": False,
        "model_type": "gemini"
    }
}

# Cache load model/tokenizer
@st.cache_resource
def load_model_and_tokenizer(model_path, tokenizer_path, model_type, token=False):
    if model_type == "gemini":
        model = genai.GenerativeModel(model_path)
        return model, None
    token_arg = None
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False) if tokenizer_path else None
    if model_type == "encoder-decoder":
        model = EncoderDecoderModel.from_pretrained(model_path, token=token_arg)
    elif model_type == "seq2seq":
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path, token=token_arg)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    model.to("cuda" if torch.cuda.is_available() else "cpu")
    return model, tokenizer

# H√†m x·ª≠ l√Ω Gemini
def generate_with_gemini(model, text, task):
    prompt = (
        f"V·ªõi t∆∞ c√°ch m·ªôt chuy√™n gia h√£y t·∫°o ti√™u ƒë·ªÅ ng·∫Øn g·ªçn cho vƒÉn b·∫£n sau: {text}" if task == "Sinh ti√™u ƒë·ªÅ"
        else f"V∆°i t∆∞ c√°ch m·ªôt chuy√™n gia h√£y t·∫°o t√≥m t·∫Øt cho vƒÉn b·∫£n: {text}"
    )
    response = model.generate_content(prompt)
    return response.text.strip()

# Init session state
if "history" not in st.session_state:
    st.session_state.history = []
if "show_sidebar" not in st.session_state:
    st.session_state.show_sidebar = False
if "selected_history_index" not in st.session_state:
    st.session_state.selected_history_index = None
if "current_generated" not in st.session_state:
    st.session_state.current_generated = None
if "current_task" not in st.session_state:
    st.session_state.current_task = None

# Sidebar
with st.sidebar:
    if st.button("üßæ Hi·ªán/·∫®n l·ªãch s·ª≠"):
        st.session_state.show_sidebar = not st.session_state.show_sidebar

if st.session_state.show_sidebar:
    with st.sidebar:
        st.markdown("### üïì L·ªãch s·ª≠")
        if not st.session_state.history:
            st.write("Ch∆∞a c√≥ l·ªãch s·ª≠ n√†o.")
        else:
            if st.button("üóëÔ∏è X√≥a t·∫•t c·∫£ l·ªãch s·ª≠"):
                st.session_state.history = []
                st.session_state.selected_history_index = None
                st.rerun()

            for idx, history_item in enumerate(st.session_state.history):
                col1, col2 = st.columns([4, 1])
                with col1:
                    # R√∫t g·ªçn c√¢u ƒë·∫ßu ƒë·ªÉ hi·ªÉn th·ªã
                    short_preview = history_item['title'].split('.')[0][:60]
                    if len(history_item['title']) > 60:
                        short_preview += "..."
                    if st.button(f"- {short_preview}", key=f"history_{idx}"):
                        st.session_state.selected_history_index = idx
                        st.session_state.current_generated = None
                with col2:
                    if st.button("üóëÔ∏è", key=f"delete_{idx}"):
                        st.session_state.history.pop(idx)
                        if st.session_state.selected_history_index == idx:
                            st.session_state.selected_history_index = None
                        st.rerun()


# M·ªôt ch√∫t CSS
st.markdown("""
    <style>
        body {
            background-color: #0e1117;
            color: #ffffff;
        }
        textarea {
            background-color: #1e1e1e !important;
            color: #ffffff !important;
            font-family: 'Courier New', monospace;
            border: 1px solid #ffffff30 !important;
            border-radius: 10px !important;
        }
        .stButton > button {
            background: linear-gradient(90deg, #4b6cb7 0%, #182848 100%);
            color: white;
            border: none;
            border-radius: 8px;
            padding: 10px 20px;
            margin-top: 10px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        .stButton > button:hover {
            background: linear-gradient(90deg, #1e3c72 0%, #2a5298 100%);
            transform: scale(1.02);
        }
        div[role="radiogroup"] label {
            margin-right: 15px;
            background-color: #2c2f36;
            padding: 8px 15px;
            border-radius: 5px;
            cursor: pointer;
        }
        div[role="radiogroup"] input:checked + label {
            background-color: #0078FF;
            color: white;
        }
        .block-container {
            padding-top: 1rem;
            padding-bottom: 1rem;
            padding-left: 2rem;
            padding-right: 2rem;
        }
        .card {
            background-color: #1e1e1e;
            padding: 20px;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
            margin-bottom: 20px;
        }
    </style>
""", unsafe_allow_html=True)

# Main App
st.markdown("""
    <h1 style='text-align: center; color: white; font-family: "Segoe UI", sans-serif;'>
        Tr√¨nh Sinh Ti√™u ƒê·ªÅ & T√≥m T·∫Øt
    </h1>
""", unsafe_allow_html=True)

task_option = st.radio(
    "Ch·ªçn ch·ª©c nƒÉng b·∫°n mu·ªën:",
    ('Sinh ti√™u ƒë·ªÅ', 'T√≥m t·∫Øt n·ªôi dung'),
    horizontal=True,
    key="task_selection"
)

selected_model_key = None
model_config = None

if task_option == 'Sinh ti√™u ƒë·ªÅ':
    selected_model_key = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh sinh ti√™u ƒë·ªÅ:",
        list(TITLE_MODELS.keys()),
        key="title_model_selector"
    )
    model_config = TITLE_MODELS[selected_model_key]

elif task_option == 'T√≥m t·∫Øt n·ªôi dung':
    selected_model_key = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh t√≥m t·∫Øt:",
        list(SUMMARIZATION_MODELS.keys()),
        key="summary_model_selector"
    )
    model_config = SUMMARIZATION_MODELS[selected_model_key]

# Upload file
uploaded_file = st.file_uploader("Ho·∫∑c t·∫£i l√™n file (.txt, .docx):", type=["txt", "docx"])

if uploaded_file:
    file_name = uploaded_file.name
    if file_name.endswith(".txt"):
        text_input = uploaded_file.read().decode("utf-8")
    elif file_name.endswith(".docx"):
        from docx import Document
        doc = Document(uploaded_file)
        text_input = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    st.text_area("N·ªôi dung file ƒë√£ t·∫£i l√™n:", value=text_input, height=200, key="text_input_area", disabled=True)
else:
    text_input = st.text_area("Nh·∫≠p ƒëo·∫°n vƒÉn c·ªßa b·∫°n:", height=200, key="text_input_area")

# N√∫t b·∫•m sau ph·∫ßn nh·∫≠p vƒÉn b·∫£n
button_label = f"{task_option}"
if st.button(button_label, key="generate_button"):
    if not model_config:
        st.warning("Vui l√≤ng ch·ªçn m√¥ h√¨nh.")
    elif not text_input.strip():
        st.warning("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ho·∫∑c t·∫£i file l√™n.")
    else:
        model, tokenizer = load_model_and_tokenizer(
            model_config["model_path"],
            model_config["tokenizer_path"],
            model_config["model_type"],
            model_config.get("token", False)
        )

        if model:
            if model_config["model_type"] == "gemini":
                processed_text = clean_text(text_input)
                try:
                    with st.spinner(f"‚è≥ ƒêang {task_option.lower()} v·ªõi m√¥ h√¨nh '{selected_model_key}'..."):
                        result = generate_with_gemini(model, processed_text, task_option)
                    
                    st.session_state.current_generated = result
                    st.session_state.current_task = task_option

                    st.session_state.history.append({
                        "title": result,
                        "input_text": text_input,
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "model_name": selected_model_key
                    })
                    st.session_state.selected_history_index = None
                    st.rerun()
                except Exception as e:
                    st.error(f"ƒê√£ x·∫£y ra l·ªói v·ªõi Gemini: {e}")
                    print(f"Error during Gemini processing: {e}")
            else:
                if model_config["model_type"] == "encoder-decoder":
                    processed_text = clean_text(text_input)
                    processed_text = segment_text(processed_text)
                else:
                    processed_text = clean_text(text_input)
                    

                try:
                    inputs = tokenizer(
                        processed_text,
                        padding="max_length",
                        truncation=True,
                        max_length=256,
                        return_tensors="pt"
                    )
                    device = "cuda" if torch.cuda.is_available() else "cpu"
                    inputs = {key: value.to(device) for key, value in inputs.items()}

                    with st.spinner(f"‚è≥ ƒêang {task_option.lower()} v·ªõi m√¥ h√¨nh '{selected_model_key}'..."):
                        with torch.no_grad():
                            outputs = model.generate(
                                inputs["input_ids"],
                                max_length=80 if task_option == 'Sinh ti√™u ƒë·ªÅ' else 200,
                                num_beams=5,
                                early_stopping=True,
                                no_repeat_ngram_size=2
                            )
                        result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
                        result = result.replace("_", " ")

                    st.session_state.current_generated = result
                    st.session_state.current_task = task_option

                    st.session_state.history.append({
                        "title": result,
                        "input_text": text_input,
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "model_name": selected_model_key
                    })
                    st.session_state.selected_history_index = None
                    st.rerun()
                except Exception as e:
                    st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")
                    print(f"Error during processing: {e}")

# Hi·ªÉn th·ªã k·∫øt qu·∫£ sinh m·ªõi
if st.session_state.current_generated:
    st.markdown("---")
    label_text = "Ti√™u ƒë·ªÅ ƒë∆∞·ª£c t·∫°o:" if st.session_state.current_task == 'Sinh ti√™u ƒë·ªÅ' else "N·ªôi dung t√≥m t·∫Øt:"
    st.markdown(f"<h3 style='color: #cccccc;'>{label_text}</h3>", unsafe_allow_html=True)
    st.markdown(f"<p style='color: white; background-color: #2a2a2a; padding: 10px; border-radius: 5px;'>"
                f"{st.session_state.current_generated}</p>", unsafe_allow_html=True)

# Hi·ªÉn th·ªã l·ªãch s·ª≠
if st.session_state.selected_history_index is not None and st.session_state.selected_history_index < len(st.session_state.history):
    selected_history = st.session_state.history[st.session_state.selected_history_index]
    st.markdown("---")
    st.markdown(f"<h3 style='color: #cccccc;'>K·∫øt qu·∫£ ƒë√£ t·∫°o:</h3>", unsafe_allow_html=True)

    if f"show_full_input_{st.session_state.selected_history_index}" not in st.session_state:
        st.session_state[f"show_full_input_{st.session_state.selected_history_index}"] = False

    show_full = st.session_state[f"show_full_input_{st.session_state.selected_history_index}"]

    input_text_to_display = selected_history['input_text'] if show_full else (selected_history['input_text'][:1000] + "..." if len(selected_history['input_text']) > 1000 else selected_history['input_text'])

    st.markdown(f"""
    <div style='color: white; background-color: #2a2a2a; padding: 10px; border-radius: 5px;'>
        <b>Model:</b> {selected_history['model_name']}<br>
        <b>Th·ªùi gian:</b> {selected_history['timestamp']}<br><br>
        <b>VƒÉn b·∫£n g·ªëc:</b><br>
        <div style='background-color: #3a3a3a; padding: 8px; border-radius: 5px; margin-bottom: 10px;'>{input_text_to_display}</div>
    """, unsafe_allow_html=True)

    if len(selected_history['input_text']) > 1000:
        if not show_full:
            if st.button("üìñ Xem ƒë·∫ßy ƒë·ªß vƒÉn b·∫£n", key=f"show_full_{st.session_state.selected_history_index}"):
                st.session_state[f"show_full_input_{st.session_state.selected_history_index}"] = True
                st.rerun()
        else:
            if st.button("üîΩ Thu g·ªçn vƒÉn b·∫£n", key=f"collapse_full_{st.session_state.selected_history_index}"):
                st.session_state[f"show_full_input_{st.session_state.selected_history_index}"] = False
                st.rerun()

    st.markdown(f"""
        <b>K·∫øt qu·∫£:</b><br>
        <div style='background-color: #3a3a3a; padding: 8px; border-radius: 5px;'>{selected_history['title']}</div>
    </div>
    """, unsafe_allow_html=True)
