import streamlit as st
from dotenv import load_dotenv
import os
import time
from datetime import datetime

from underthesea import word_tokenize
from transformers import EncoderDecoderModel, AutoModelForSeq2SeqLM, AutoTokenizer
import torch
import logging
import transformers

# Gi·∫£m b·ªõt c·∫£nh b√°o
logging.getLogger('streamlit.runtime.scriptrunner.script_run_context').setLevel(logging.ERROR)
transformers.logging.set_verbosity_error()

# C·∫•u h√¨nh Streamlit
st.set_page_config(page_title="Tr√¨nh sinh ti√™u ƒë·ªÅ", layout="centered")

# Load bi·∫øn m√¥i tr∆∞·ªùng
torch.classes.__path__ = []
load_dotenv()
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

# C√°c m√¥ h√¨nh
TITLE_MODELS = {
    "PhoBERT Encoder-Decoder": {
        "model_path": "PuppetLover/Title_generator",
        "tokenizer_path": "PuppetLover/Finetune_PhoBert",
        "use_auth_token": True,
        "model_type": "encoder-decoder"
    },
    "ViT5 Title Generator": {
        "model_path": "HTThuanHcmus/vit5-base-vietnews-summarization-finetune",
        "tokenizer_path": "HTThuanHcmus/vit5-base-vietnews-summarization-finetune",
        "use_auth_token": False,
        "model_type": "seq2seq"
    },
    "BARTpho Title Generator": {
        "model_path": "HTThuanHcmus/bartpho-finetune",
        "tokenizer_path": "HTThuanHcmus/bartpho-finetune",
        "use_auth_token": False,
        "model_type": "seq2seq"
    }
}

SUMMARIZATION_MODELS = {
    "ViT5 Summarization": {
        "model_path": "HTThuanHcmus/vit5-base-vietnews-summarization-finetune",
        "tokenizer_path": "HTThuanHcmus/vit5-base-vietnews-summarization-finetune",
        "use_auth_token": False,
        "model_type": "seq2seq"
    }
}

# Cache load model/tokenizer
@st.cache_resource
def load_model_and_tokenizer(model_path, tokenizer_path, model_type, use_auth_token=False):
    token_arg = HUGGINGFACE_TOKEN if use_auth_token and HUGGINGFACE_TOKEN else None
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)
    if model_type == "encoder-decoder":
        model = EncoderDecoderModel.from_pretrained(model_path, use_auth_token=token_arg)
    elif model_type == "seq2seq":
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path, use_auth_token=token_arg)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    model.to("cuda" if torch.cuda.is_available() else "cpu")
    return model, tokenizer

# Init session state
if "history" not in st.session_state:
    st.session_state.history = []
if "show_sidebar" not in st.session_state:
    st.session_state.show_sidebar = False
if "selected_history_index" not in st.session_state:
    st.session_state.selected_history_index = None
if "current_generated" not in st.session_state:
    st.session_state.current_generated = None
if "current_task" not in st.session_state:
    st.session_state.current_task = None

# Sidebar
with st.sidebar:
    if st.button("üßæ Hi·ªán/·∫®n l·ªãch s·ª≠"):
        st.session_state.show_sidebar = not st.session_state.show_sidebar

if st.session_state.show_sidebar:
    with st.sidebar:
        st.markdown("### üïì L·ªãch s·ª≠")
        if not st.session_state.history:
            st.write("Ch∆∞a c√≥ l·ªãch s·ª≠ n√†o.")
        else:
            if st.button("üóëÔ∏è X√≥a t·∫•t c·∫£ l·ªãch s·ª≠"):
                st.session_state.history = []
                st.session_state.selected_history_index = None
                st.rerun()

            for idx, history_item in enumerate(st.session_state.history):
                col1, col2 = st.columns([4, 1])
                with col1:
                    if st.button(f"- {history_item['title']}", key=f"history_{idx}"):
                        st.session_state.selected_history_index = idx
                        st.session_state.current_generated = None
                with col2:
                    if st.button("üóëÔ∏è", key=f"delete_{idx}"):
                        st.session_state.history.pop(idx)
                        if st.session_state.selected_history_index == idx:
                            st.session_state.selected_history_index = None
                        st.rerun()

# M·ªôt ch√∫t css
st.markdown("""
    <style>
        textarea {
            background-color: #1e1e1e !important;
            color: white !important;
        }
        .stButton > button {
            background-color: #333333;
            color: white;
            border: 1px solid #ffffff30;
            border-radius: 8px;
            width: 100%;
            margin-top: 10px;
        }
        .stButton > button:hover {
            background-color: #444444;
        }
        div[role="radiogroup"] label {
            margin-right: 15px;
        }
    </style>
""", unsafe_allow_html=True)

# Main App
st.markdown("<h1 style='text-align: center; color: white;'>Tr√¨nh sinh ti√™u ƒë·ªÅ v√† t√≥m t·∫Øt</h1>", unsafe_allow_html=True)

task_option = st.radio(
    "Ch·ªçn ch·ª©c nƒÉng b·∫°n mu·ªën:",
    ('Sinh ti√™u ƒë·ªÅ', 'T√≥m t·∫Øt n·ªôi dung'),
    horizontal=True,
    key="task_selection"
)

selected_model_key = None
model_config = None

if task_option == 'Sinh ti√™u ƒë·ªÅ':
    selected_model_key = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh sinh ti√™u ƒë·ªÅ:",
        list(TITLE_MODELS.keys()),
        key="title_model_selector"
    )
    model_config = TITLE_MODELS[selected_model_key]

elif task_option == 'T√≥m t·∫Øt n·ªôi dung':
    selected_model_key = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh t√≥m t·∫Øt:",
        list(SUMMARIZATION_MODELS.keys()),
        key="summary_model_selector"
    )
    model_config = SUMMARIZATION_MODELS[selected_model_key]

# Upload file
uploaded_file = st.file_uploader("Ho·∫∑c t·∫£i l√™n file (.txt, .docx):", type=["txt", "docx"])

if uploaded_file:
    file_name = uploaded_file.name
    if file_name.endswith(".txt"):
        text_input = uploaded_file.read().decode("utf-8")
    elif file_name.endswith(".docx"):
        from docx import Document
        doc = Document(uploaded_file)
        text_input = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

    st.text_area("N·ªôi dung file ƒë√£ t·∫£i l√™n:", value=text_input, height=200, key="text_input_area", disabled=True)
else:
    text_input = st.text_area("Nh·∫≠p ƒëo·∫°n vƒÉn c·ªßa b·∫°n:", height=200, key="text_input_area")

# N√∫t b·∫•m sau ph·∫ßn nh·∫≠p vƒÉn b·∫£n
button_label = f"{task_option}"
if st.button(button_label, key="generate_button"):
    if not model_config:
        st.warning("Vui l√≤ng ch·ªçn m√¥ h√¨nh.")
    elif not text_input.strip():
        st.warning("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ho·∫∑c t·∫£i file l√™n.")
    else:
        model, tokenizer = load_model_and_tokenizer(
            model_config["model_path"],
            model_config["tokenizer_path"],
            model_config["model_type"],
            model_config.get("use_auth_token", False)
        )

        if model and tokenizer:
            processed_text = word_tokenize(text_input, format="text")

            try:
                inputs = tokenizer(
                    processed_text,
                    padding="max_length",
                    truncation=True,
                    max_length=256,
                    return_tensors="pt"
                )
                device = "cuda" if torch.cuda.is_available() else "cpu"
                inputs = {key: value.to(device) for key, value in inputs.items()}

                progress_message = st.empty()
                progress_message.info(f"ƒêang {task_option.lower()} v·ªõi m√¥ h√¨nh: '{selected_model_key}'...")

                with torch.no_grad():
                    outputs = model.generate(
                        inputs["input_ids"],
                        max_length=80 if task_option == 'Sinh ti√™u ƒë·ªÅ' else 200,
                        num_beams=5,
                        early_stopping=True,
                        no_repeat_ngram_size=2
                    )

                result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

                st.session_state.current_generated = result
                st.session_state.current_task = task_option

                st.session_state.history.append({
                    "title": result,
                    "input_text": text_input,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "model_name": selected_model_key
                })
                st.session_state.selected_history_index = None

                progress_message.empty()

                st.rerun()

            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")
                print(f"Error during processing: {e}")

# Hi·ªÉn th·ªã k·∫øt qu·∫£ sinh m·ªõi
if st.session_state.current_generated:
    st.markdown("---")
    label_text = "Ti√™u ƒë·ªÅ ƒë∆∞·ª£c t·∫°o:" if st.session_state.current_task == 'Sinh ti√™u ƒë·ªÅ' else "N·ªôi dung t√≥m t·∫Øt:"
    st.markdown(f"<h3 style='color: #cccccc;'>{label_text}</h3>", unsafe_allow_html=True)
    st.markdown(f"<p style='color: white; background-color: #2a2a2a; padding: 10px; border-radius: 5px;'>"
                f"{st.session_state.current_generated}</p>", unsafe_allow_html=True)

# Hi·ªÉn th·ªã l·ªãch s·ª≠
if st.session_state.selected_history_index is not None and st.session_state.selected_history_index < len(st.session_state.history):
    selected_history = st.session_state.history[st.session_state.selected_history_index]
    st.markdown("---")
    st.markdown(f"<h3 style='color: #cccccc;'>K·∫øt qu·∫£ ƒë√£ t·∫°o:</h3>", unsafe_allow_html=True)

    if f"show_full_input_{st.session_state.selected_history_index}" not in st.session_state:
        st.session_state[f"show_full_input_{st.session_state.selected_history_index}"] = False

    show_full = st.session_state[f"show_full_input_{st.session_state.selected_history_index}"]

    input_text_to_display = selected_history['input_text'] if show_full else (selected_history['input_text'][:1000] + "..." if len(selected_history['input_text']) > 1000 else selected_history['input_text'])

    st.markdown(f"""
    <div style='color: white; background-color: #2a2a2a; padding: 10px; border-radius: 5px;'>
        <b>Model:</b> {selected_history['model_name']}<br>
        <b>Th·ªùi gian:</b> {selected_history['timestamp']}<br><br>
        <b>VƒÉn b·∫£n g·ªëc:</b><br>
        <div style='background-color: #3a3a3a; padding: 8px; border-radius: 5px; margin-bottom: 10px;'>{input_text_to_display}</div>
    """, unsafe_allow_html=True)

    if len(selected_history['input_text']) > 1000:
        if not show_full:
            if st.button("üìñ Xem ƒë·∫ßy ƒë·ªß vƒÉn b·∫£n", key=f"show_full_{st.session_state.selected_history_index}"):
                st.session_state[f"show_full_input_{st.session_state.selected_history_index}"] = True
                st.rerun()
        else:
            if st.button("üîΩ Thu g·ªçn vƒÉn b·∫£n", key=f"collapse_full_{st.session_state.selected_history_index}"):
                st.session_state[f"show_full_input_{st.session_state.selected_history_index}"] = False
                st.rerun()

    st.markdown(f"""
        <b>K·∫øt qu·∫£:</b><br>
        <div style='background-color: #3a3a3a; padding: 8px; border-radius: 5px;'>{selected_history['title']}</div>
    </div>
    """, unsafe_allow_html=True)
